# Course 2, week 1: Practical Aspects of Deep Learning

* 训练深度学习模型是一项 empirical 的任务:

![img/idea_code_experiment.png](idea_code_experiment.png)

* 传统机器学习由于数据量比较小, 通常将数据分成 7:3 这样的 train/test 集, 或 6:2:2 这样的 train/dev/test 集; 但深度学习由于用到大量的数据, 不必为 dev 或 test 分配太多的数据, 应将绝大多数数据用于 train, 因此一般性的数据划分可以是 98:1:1 或 99.5:0.4:0.1 这样的.
* training set 与 dev/test 的数据分布可以不一样, 但务必确保 dev 与 test 的数据具有相同的分布.
* 设立 dev 的目的是, 从训练得到的多个模型中找到最优的模型, 而 test 的目的是检验选择的模型的泛化能力. 因此可以不要 test set. 上述的 train/test, 更确切的说法应为 train/dev.
* `高偏差 high bias` 对应`欠拟合 underfit`, `高方差 high variance` 对应 `过拟合 overfit`.
* 训练误差远大于`人类水平 human level`, 意味着高偏差, 欠拟合; 检验误差与训练误差很大, 意味着高方差, 过拟合. (对于某项任务, 人类误差也很大的情况下, 训练误差很大, 但接近人类水平, 不称为欠拟合) (训练误差与人类水平 (误差) 之间的差距, 吴恩达老师称为`可避免偏差, avoidable bias`)
* basic recipe for machine learning

![img/basic_recipe_for_machine_learning.png](basic_recipe_for_machine_learning.png)

* 如上图所示, 首先要保证 low bias (考究的是训练集的性能), 如果偏差很大, 尝试更大的神经网络, 比如训练更长时间, 修改模型; 在保证了 low bias 之后, 要检验 variance, 即验证集的性能, 如果方差很大, 可以尝试更多的验证数据, 或者通过`正则项 regularization` 引入惩罚. 最终得到低偏差低方差的模型
* 减小偏差的技术, 可能会引起方差增大; 反之亦然. 这称为 `偏差-方差平衡 bias-variance trade-off`
* 引入正则项, 实际上是引入了对参数 (w, b) 的新的限制. 比如增加了 L2 正则项的成本函数, 最小化成本函数, 所求的是 L2 范数限制下的参数. 以下是一张很好的示意图:

![img/L2_regularization_illustration.png](L2_regularization_illustration.png)
<p>图片截自"Python Machine Learning - Sebastian Raschka"</p>

* 上图中以交点作为最优的参数值的数学依据是: `a+b>=2 sqrt(ab), 当且仅当 a=b 时, 等号成立`
* 添加 L1 正则项, 将得到稀疏向量 (可用于特征选择), 示意图如下:

![img/L1_regularization_illustration.png](L1_regularization_illustration.png)
<p>图片截自"Python Machine Learning - Sebastian Raschka"</p>

* `L2 正则化`有时候也被称为 `weights decay (权重衰减)`. 因为在添加了 L2 正则项的情况下, 反向传播时, dW^[l] 也多了一项 `+lambda*W^[l]/m`. 权值更新如下所示. `1-alpha*lambda/m < 1`, weights decay 因此得名:

![img/weights_decay.png](weights_decay.png)

* 正则化能够防止过拟合, 一个理解角度是: 添加了正则项之后, 模型将更偏好于较小的权值. 假设正则化系数 lambda 足够大, 那么权值将趋向于 0, 这相当于闭掉了一些神经元, 模型更简单了, 由参数确定的分割超平面更平滑.

![img/regularization_prevent_overfitting.png](regularization_prevent_overfitting.png)

* 正则化能够防止过拟合, 另一个理解角度是: 以 tanh 激活函数为例, 随着正则化系数 lambda 增大, 权值 W^[l] 减小, 则 z^[l]=W^[l]a^[l-1]+b^[l] 也减小 (暂时忽略 b), 对于 tanh 函数而言, 在 z 较小的情况下, 函数几乎是线性的. 大量线性单元构成的神经网络, 也可以简化为一个线性模型. 由此, 正则化后的分割超平面也就更平滑了, 防止了过拟合.

![img/regularization_prevent_overfitting_2.png](regularization_prevent_overfitting_2.png)

* `Dropout regularization` 对每个样本进行训练时, 每一层的每个神经元都以一定概率关闭, 这样每次训练都得到一个较小的神经网络, 以这个较小的神经网络对样本进行学习, 从而起到正则化的作用.
* 一种最常用的 Dropout regularization 是 `Inverted Dropout`: 该算法对每一层, 首先随机初始化一个与 a 大小相同的向量, 根据每个元素与固定概率 k 的关系, 得到一个布尔向量 d (或者说 0-1 二值向量), 然后原激活函数值向量 a 乘以 d, 得到新向量 a, 由于 d 中存在 0 元素, 原 a 向量中的元素乘以 0, 结果还是 0, 相当于被屏蔽了. 最后为了保持该层输出不变, 变换后的 a 向量除以 k, 弥补"屏蔽"带来的损失.
* 不同于一般的正则化, Dropout regualarization 只在训练时使用, 测试预测时, 不使用.
* 对于 Dropout regularization 防止过拟合的一种理解是: 任一特征都是不可靠的, 每次都对上一层所有输出的特征进行学习是很勉强的 (reluctant), 因此, 每次关停某一层的一些神经元, 下一层神经元的输入就减少了, 从而避免了对不可靠 (以随机概率走远) 特征的学习, 从而避免了过拟合.
* 与 L2 正则化类似, Dropout regularization 起到了 shrink weights 的作用, 关停上一层某一神经元, 相当于其对应的权值也无效了.
* Dropout 正则化, 可以为每一层独立设置`保留 (神经元) 概率 keep probability`, 增加了灵活性. 对于很可能会过拟合的层, 可以设置较小的 keep_prob. 一般不对输入层进行 drop out.
* Dropout 正则化的一个缺点是, 很难去定义成本函数 J, 因此每次都随机关停了一些节点. 对此, 比较好的做法是, 在执行 Dropout 正则化之前, 先观察成本函数随时间的变化, 确保无误之后, 再进行 Dropout 正则化.
* `数据增强 Data augmentation` 是一种在需要更多训练数据时, 可以采取的技术手段. 它对现有的一份样本, 进行一定程度的变换 (比如图片的对称变换, 旋转变换), 得到新的样本.
* `早停 early stopping` 也是一种防止过拟合的技术手段. 训练开始时, 训练误差和检验误差都随着迭代的进行而下降, 训练误差理论上会持续下降, 而检验误差在达到某一最小值之后, 随着迭代的继续, 反而会上升. 此时, 应在检验误差达到最小值的时候提前结束迭代, 这就是 early stopping.

![img/early_stopping.png](early_stopping.png)

* `正规化 normalizing` 将不同尺度的特征缩放到同一尺度 (每个样本都减去样本均值, 再除以样本方差), 比如 [0, 1] 或 [-1, 1] 的范围. 带来的一个好处是, 使梯度下降算法更快收敛.

![img/normalize_accelarate_gradient_descent.png](normalize_accelarate_gradient_descent.png)

* `梯度爆炸/消失, vanishing/exploding gradients`, 对于一个非常深的深度神经网络, 忽略 b, 仅考虑参数 w, 当每一层的权值都略大于 1 或单位矩阵, 前向传播到最后, 激活函数值将爆炸 (因为指数级增长); 当每一层的权值都略小与 1 或单位矩阵时, 到最后, 激活函数值将无限小 (指数级衰减). 这从一方面证明了随机初始化参数的重要性.
* 对于单个神经元, `z=w.T x+b`, 一个直观的感觉, 输入特征数 n 越大, z 也会越大. 为了防止 z 过大或过小, 一种方式是选择更小的权值, 通常的做法是, 初始化权值时, 使权值的`方差 Variance` Var(wi) 等于一个较小的值, 比如 1/n:
    1. 对于 ReLU 激活函数, 权值通常初始化为: `np.random.randn(shape) * np.sqrt(2/n)`
    1. 对于 tanh 激活函数, 权值通常初始化为: `np.random.randn(shape) * np.sqrt(1/n)`. 也称为 `Xavier 初始化`
    1. 另外还有一种权值初始化的方式是: `np.random.randn(shape) * np.sqrt(2/(n + o))`  其中 o 为输出特征数
* 所谓`梯度检查 gradient checking`, 就是检验微分是否正确. 通常的做法是, 利用极限原理, 设 g(theta) 是 f(theta) 的导数, 则梯度检查就是验证 `(f(theta + epsilon) - f(theta))/epsilon` 是否很接近 g(theta), 或者是 `(f(theta + epsilon) - f(theta - epsilon))/(2*epsilon)`, 精度会更高. 一般而言, 误差在 10^-7 以内的, 表明微分没问题; 误差在 10^-7 ~ 10^-5 的, 可能有点问题; 误差在 10^-3 的, 很有可能微分出错了.
* Gradient Checking 的一些注意点:
    - 只在调试的时候用, 训练时不需要
    - 梯度检查失败, 说明算法肯定存在问题
    - 别忘了正则化
    - 梯度检查对 Dropout 无效, 因为 Dropout 随机关停神经元, 很难对成本函数进行定义
    - 随机初始化
